# -*- coding: utf-8 -*-
"""Simple Linear Regression of Exponential function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vaPvzQuHxPKUJ6cq6zsiQsQbdu_36ouW
"""

#Importing Library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import LearningRateScheduler

#Downloading the dataset
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv

#Loading the dataset
data = pd.read_csv('moore.csv',header=None)

data

#Convert to an Numpy array
data = np.array(data)

data

#Split the dataset into X(Independent variables) and y(output/dependent variables)
X = data[:,0].reshape(-1,1)#Reshaping to turn the train data into 2d(N,dim) as it iis now only 1d(N)
y = data[:,1]

X.shape

#Scatter plot for X and y
plt.scatter(X,y)

#As you can see an exponential growth for output, we must convert it to Linear distribution for Linear regression to fit
#Inorder to convert the output(y) to Linear distribution, you can take the log(y)
y = np.log(y)

#Now, let's plot again(Linear Distribution has taken place)
plt.scatter(X,y)

#Fitting over a 50 years as input is not good,so one must convert it to a standard distribution over a value
#You can perform standard deviation for a normal Distribution(It depends how one want his data to be)
X = X - X.mean()

#You can now see there is a normal distribution for predictor
plt.scatter(X,y)

#Initialize the model
model = Sequential()
model.add(Dense(1,input_dim=1,)) #Input and Output neuron units just 1 as we need only 1 value. No need of any activation function as we require continous distribution. input_dim=provide the dimension of input data(here 1)

model.summary()

#Initialize a Learning Rate Scheduler
def schedule(epochs,lr):
  if epochs>50:
    return 0.0001
  return 0.001

#Compile the model with hyperparameters
model.compile(optimizer='SGD',loss='mse')

#Calling the Learning Scheduler
scheduler = LearningRateScheduler(schedule)

#Fit the model
r = model.fit(X,y,epochs=200,callbacks=[scheduler])

#Do not worry about the loss mentioned here(as it is Linear Regression)
#Plot the loss
plt.plot(r.history['loss'])

#Lets get the slope(weights) and intercept(bias) of the Linear Equation
slope = model.layers[0].get_weights()[0][0][0]
intercept = model.layers[0].get_weights()[1][0]
print("Slope(m): ",slope)
print("intercept(b): ",intercept)

